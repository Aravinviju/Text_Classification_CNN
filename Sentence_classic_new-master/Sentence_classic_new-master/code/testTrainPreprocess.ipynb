{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "# import numpy as np\n",
    "# import gzip\n",
    "# import os\n",
    "# import sys\n",
    "# if (sys.version_info > (3, 0)):\n",
    "#     import pickle as pkl\n",
    "# else: #Python 2.7 imports\n",
    "#     import cPickle as pkl\n",
    "\n",
    "# #We download English word embeddings from here https://www.cs.york.ac.uk/nlp/extvec/\n",
    "# embeddingsPath = '/home/apoopat1/Arav/Sentence_classic/code/embeddings/wiki_extvec.gz'\n",
    "\n",
    "# #Train, Dev, and Test files\n",
    "# folder = '/home/apoopat1/Arav/Sentence_classic/code/data/'\n",
    "# files = [folder+'train.txt',  folder+'dev.txt', folder+'test.txt']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def createMatrices(sentences, word2Idx):\n",
    "#     unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "#     paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "#     xMatrix = []\n",
    "#     unknownWordCount = 0\n",
    "#     wordCount = 0\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         targetWordIdx = 0\n",
    "        \n",
    "#         sentenceWordIdx = []\n",
    "        \n",
    "#         for word in sentence:\n",
    "#             wordCount += 1\n",
    "            \n",
    "#             if word in word2Idx:\n",
    "#                 wordIdx = word2Idx[word]\n",
    "#             elif word.lower() in word2Idx:\n",
    "#                 wordIdx = word2Idx[word.lower()]\n",
    "#             else:\n",
    "#                 wordIdx = unknownIdx\n",
    "#                 unknownWordCount += 1\n",
    "                \n",
    "#             sentenceWordIdx.append(wordIdx)\n",
    "            \n",
    "#         xMatrix.append(sentenceWordIdx)\n",
    "       \n",
    "    \n",
    "#     print(\"Unknown tokens: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "#     return xMatrix\n",
    "\n",
    "# def readFile(filepath):\n",
    "#     sentences = []    \n",
    "#     labels = []\n",
    "    \n",
    "#     for line in open(filepath):   \n",
    "#         splits = line.split()\n",
    "#         label = int(splits[0])\n",
    "#         words = splits[1:]\n",
    "        \n",
    "#         labels.append(label)\n",
    "#         sentences.append(words)\n",
    "        \n",
    "#     print(filepath, len(sentences), \"sentences\")\n",
    "    \n",
    "#     return sentences, labels\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# # ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "# #      Start of the preprocessing\n",
    "# # ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "\n",
    "# outputFilePath = '/home/apoopat1/Arav/Sentence_classic/code/pkl/data.pkl.gz'\n",
    "\n",
    "\n",
    "# trainDataset = readFile(files[0])\n",
    "# devDataset = readFile(files[1])\n",
    "# testDataset = readFile(files[2])\n",
    "\n",
    "\n",
    "# # :: Compute which words are needed for the train/dev/test set ::\n",
    "# words = {}\n",
    "# for sentences, labels in [trainDataset, devDataset, testDataset]:       \n",
    "#     for sentence in sentences:\n",
    "#         for token in sentence:\n",
    "#             words[token.lower()] = True\n",
    "\n",
    "\n",
    "# # :: Read in word embeddings ::\n",
    "# word2Idx = {}\n",
    "# wordEmbeddings = []\n",
    "\n",
    "# # :: Downloads the embeddings from the York webserver ::\n",
    "# if not os.path.isfile(embeddingsPath):\n",
    "#     basename = os.path.basename(embeddingsPath)\n",
    "#     if basename == 'wiki_extvec.gz':\n",
    "# \t       print(\"Start downloading word embeddings for English using wget ...\")\n",
    "# \t       #os.system(\"wget https://www.cs.york.ac.uk/nlp/extvec/\"+basename+\" -P embeddings/\") #Original path from York University\n",
    "# \t       os.system(\"wget https://public.ukp.informatik.tu-darmstadt.de/reimers/2017_english_embeddings/\"+basename+\" -P embeddings/\")\n",
    "#     else:\n",
    "#         print(embeddingsPath, \"does not exist. Please provide pre-trained embeddings\")\n",
    "#         exit()\n",
    "        \n",
    "# # :: Load the pre-trained embeddings file ::\n",
    "# fEmbeddings = gzip.open(embeddingsPath, \"r\") if embeddingsPath.endswith('.gz') else open(embeddingsPath, encoding=\"utf8\")\n",
    "\t\n",
    "# print(\"Load pre-trained embeddings file\")\n",
    "# for line in fEmbeddings:\n",
    "#     split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "#     word = split[0]\n",
    "    \n",
    "#     if len(word2Idx) == 0: #Add padding+unknown\n",
    "#         word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "#         vector = np.zeros(len(split)-1) #Zero vector for 'PADDING' word\n",
    "#         wordEmbeddings.append(vector)\n",
    "        \n",
    "#         word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "#         vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "#         wordEmbeddings.append(vector)\n",
    "\n",
    "#     if word.lower() in words:\n",
    "#         vector = np.array([float(num) for num in split[1:]])\n",
    "#         wordEmbeddings.append(vector)\n",
    "#         word2Idx[word] = len(word2Idx)\n",
    "       \n",
    "        \n",
    "# wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "# print(\"Embeddings shape: \", wordEmbeddings.shape)\n",
    "# print(\"Len words: \", len(words))\n",
    "\n",
    "\n",
    "\n",
    "# # :: Create matrices ::\n",
    "# train_matrix = createMatrices(trainDataset[0], word2Idx)\n",
    "# dev_matrix = createMatrices(devDataset[0], word2Idx)\n",
    "# test_matrix = createMatrices(testDataset[0], word2Idx)\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     'wordEmbeddings': wordEmbeddings, 'word2Idx': word2Idx,\n",
    "#     'train': {'sentences': train_matrix, 'labels': trainDataset[1]},\n",
    "#     'dev':   {'sentences': dev_matrix, 'labels': devDataset[1]},\n",
    "#     'test':  {'sentences': test_matrix, 'labels': testDataset[1]}\n",
    "#     }\n",
    "\n",
    "\n",
    "# f = gzip.open(outputFilePath, 'wb')\n",
    "# pkl.dump(data, f)\n",
    "# f.close()\n",
    "\n",
    "# print(\"Data stored in pkl folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We download English word embeddings from here https://www.cs.york.ac.uk/nlp/extvec/\n",
    "embeddingsPath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/embeddings/wiki_extvec.gz'\n",
    "\n",
    "#Train, Dev, and Test files\n",
    "folder = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/'\n",
    "files = [folder+'mailtrain.txt',  folder+'maildev.txt', folder+'mailtest.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    xMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        targetWordIdx = 0\n",
    "        \n",
    "        sentenceWordIdx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            wordCount += 1\n",
    "            \n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "                \n",
    "            sentenceWordIdx.append(wordIdx)\n",
    "            \n",
    "        xMatrix.append(sentenceWordIdx)\n",
    "       \n",
    "    \n",
    "    print(\"Unknown tokens: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "    return xMatrix\n",
    "\n",
    "def readFile(filepath):\n",
    "    sentences = []    \n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath, encoding=\"utf-8\"):   \n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/mailtrain.txt 8094 sentences\n",
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/maildev.txt 4047 sentences\n",
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/mailtest.txt 4048 sentences\n"
     ]
    }
   ],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "#      Start of the preprocessing\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "\n",
    "outputFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/maildata.pkl.gz'\n",
    "\n",
    "\n",
    "trainDataset = readFile(files[0])\n",
    "devDataset = readFile(files[1])\n",
    "testDataset = readFile(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Compute which words are needed for the train/dev/test set ::\n",
    "words = {}\n",
    "for sentences, labels in [trainDataset, devDataset, testDataset]:       \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            words[token.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDataset\n",
    "# words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "# :: Downloads the embeddings from the York webserver ::\n",
    "if not os.path.isfile(embeddingsPath):\n",
    "    basename = os.path.basename(embeddingsPath)\n",
    "    if basename == 'wiki_extvec.gz':\n",
    "           print(\"Start downloading word embeddings for English using wget ...\")\n",
    "           #os.system(\"wget https://www.cs.york.ac.uk/nlp/extvec/\"+basename+\" -P embeddings/\") #Original path from York University\n",
    "           os.system(\"wget https://public.ukp.informatik.tu-darmstadt.de/reimers/2017_english_embeddings/\"+basename+\" -P embeddings/\")\n",
    "    else:\n",
    "        print(embeddingsPath, \"does not exist. Please provide pre-trained embeddings\")\n",
    "        exit()\n",
    "        \n",
    "# :: Load the pre-trained embeddings file ::\n",
    "fEmbeddings = gzip.open(embeddingsPath, \"r\") if embeddingsPath.endswith('.gz') else open(embeddingsPath, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained embeddings file\n",
      "Embeddings shape:  (7290, 300)\n",
      "Len words:  11720\n"
     ]
    }
   ],
   "source": [
    "print(\"Load pre-trained embeddings file\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector for 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if word.lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)\n",
    "       \n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "print(\"Embeddings shape: \", wordEmbeddings.shape)\n",
    "print(\"Len words: \", len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens: 8.35%\n",
      "Unknown tokens: 8.20%\n",
      "Unknown tokens: 8.33%\n",
      "Data stored in pkl folder\n"
     ]
    }
   ],
   "source": [
    "# :: Create matrices ::\n",
    "train_matrix = createMatrices(trainDataset[0], word2Idx)\n",
    "dev_matrix = createMatrices(devDataset[0], word2Idx)\n",
    "test_matrix = createMatrices(testDataset[0], word2Idx)\n",
    "\n",
    "\n",
    "data = {\n",
    "    'wordEmbeddings': wordEmbeddings, 'word2Idx': word2Idx,\n",
    "    'train': {'sentences': train_matrix, 'labels': trainDataset[1]},\n",
    "    'dev':   {'sentences': dev_matrix, 'labels': devDataset[1]},\n",
    "    'test':  {'sentences': test_matrix, 'labels': testDataset[1]}\n",
    "    }\n",
    "\n",
    "\n",
    "f = gzip.open(outputFilePath, 'wb')\n",
    "pkl.dump(data, f)\n",
    "f.close()\n",
    "\n",
    "print(\"Data stored in pkl folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
