{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/mailtrain.txt 8094 sentences\n",
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/maildev.txt 4047 sentences\n",
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/mailtest.txt 4048 sentences\n",
      "Load pre-trained embeddings file\n",
      "Embeddings shape:  (7290, 300)\n",
      "Len words:  11720\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl\n",
    "    \n",
    "\n",
    "#We download English word embeddings from here https://www.cs.york.ac.uk/nlp/extvec/\n",
    "embeddingsPath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/embeddings/wiki_extvec.gz'\n",
    "\n",
    "#Train, Dev, and Test files\n",
    "folder = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/'\n",
    "files = [folder+'mailtrain.txt',  folder+'maildev.txt', folder+'mailtest.txt']\n",
    "\n",
    "\n",
    "def createMatrices(sentences, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    xMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        targetWordIdx = 0\n",
    "        \n",
    "        sentenceWordIdx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            wordCount += 1\n",
    "            \n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "                \n",
    "            sentenceWordIdx.append(wordIdx)\n",
    "            \n",
    "        xMatrix.append(sentenceWordIdx)\n",
    "       \n",
    "    \n",
    "    print(\"Unknown tokens: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "    return xMatrix\n",
    "\n",
    "def readFile(filepath):\n",
    "    sentences = []    \n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath, encoding = \"utf-8\"):   \n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "#      Start of the preprocessing\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "\n",
    "outputFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/maildata.pkl.gz'\n",
    "\n",
    "\n",
    "trainDataset = readFile(files[0])\n",
    "devDataset = readFile(files[1])\n",
    "testDataset = readFile(files[2])\n",
    "\n",
    "\n",
    "# :: Compute which words are needed for the train/dev/test set ::\n",
    "words = {}\n",
    "for sentences, labels in [trainDataset, devDataset, testDataset]:       \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            words[token.lower()] = True\n",
    "\n",
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "# :: Downloads the embeddings from the York webserver ::\n",
    "if not os.path.isfile(embeddingsPath):\n",
    "    basename = os.path.basename(embeddingsPath)\n",
    "    if basename == 'wiki_extvec.gz':\n",
    "\t       print(\"Start downloading word embeddings for English using wget ...\")\n",
    "\t       #os.system(\"wget https://www.cs.york.ac.uk/nlp/extvec/\"+basename+\" -P embeddings/\") #Original path from York University\n",
    "\t       os.system(\"wget https://public.ukp.informatik.tu-darmstadt.de/reimers/2017_english_embeddings/\"+basename+\" -P embeddings/\")\n",
    "    else:\n",
    "        print(embeddingsPath, \"does not exist. Please provide pre-trained embeddings\")\n",
    "        exit()\n",
    "        \n",
    "\n",
    "# :: Load the pre-trained embeddings file ::\n",
    "fEmbeddings = gzip.open(embeddingsPath, \"r\") if embeddingsPath.endswith('.gz') else open(embeddingsPath, encoding=\"utf8\")\n",
    "\n",
    "\n",
    "print(\"Load pre-trained embeddings file\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector for 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if word.lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)\n",
    "       \n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "print(\"Embeddings shape: \", wordEmbeddings.shape)\n",
    "print(\"Len words: \", len(words))\n",
    "\n",
    "\n",
    "word2Idx_path = open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/word2Idx.pickle\",\"wb\")\n",
    "pkl.dump(word2Idx,word2Idx_path)\n",
    "word2Idx_path.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual speech rate is:  125\n",
      "Energy threshold - before:  300\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------speech to text -> input text:\n",
    "\n",
    "# import speech_recognition\n",
    "# import pyttsx3\n",
    "\n",
    "# speech_engine = pyttsx3.init('espeak') # see http://pyttsx.readthedocs.org/en/latest/engine.html#pyttsx.init\n",
    "# print(\"Actual speech rate is: \",speech_engine.getProperty('rate'))\n",
    "# speech_engine.setProperty('rate', 125)\n",
    "\n",
    "# def speak(text):\n",
    "#     speech_engine.say(text)\n",
    "#     speech_engine.runAndWait()\n",
    "\n",
    "# recognizer = speech_recognition.Recognizer()\n",
    "\n",
    "# print(\"Energy threshold - before: \", recognizer.energy_threshold)\n",
    "\n",
    "\n",
    "\n",
    "# def listen():\n",
    "# #     with speech_recognition.AudioFile(\"/home/dl1/Arav/Sentence_classic/code/newtest.wav\") as source:\n",
    "#     with speech_recognition.Microphone() as source:\n",
    "#         recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "#         recognizer.dynamic_energy_threshold = False\n",
    "# #         print(\"Chunking rate:\", source.CHUNK)\n",
    "# #         print(\"format rate:\", source.format)\n",
    "#         recognizer.energy_threshold =400\n",
    "#         print(\"Energy threshold - after: \", recognizer.energy_threshold)\n",
    "\n",
    "#         audio = recognizer.listen(source)\n",
    "        \n",
    "# #         audio = recognizer.record(source)\n",
    "\n",
    "#     try:\n",
    "#         text = recognizer.recognize_google(audio, language='en-GB')\n",
    "#         print(text)\n",
    "#         return text\n",
    "    \n",
    "# #         or:return recognizer.recognize_sphinx(audio)\n",
    "        \n",
    "#     except speech_recognition.UnknownValueError:\n",
    "#         print(\"Could not understand audio\")\n",
    "# #         reply = \"sorry, I could not understand that\"\n",
    "# #         return reply\n",
    "#     except speech_recognition.RequestError as e:\n",
    "#         print(\"Recog Error; {0}\".format(e))\n",
    "\n",
    "#     return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12311574173532944334\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 170459136\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 10317876230059822873\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 338231296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16976386436290909991\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Energy threshold - after:  400\n",
      "tackle\n",
      "Unknown tokens in test_Text: 100.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.25253412]\n",
      "The user is negative about:  [['tackle']]\n",
      "Energy threshold - after:  400\n",
      "ok right yeah\n",
      "Unknown tokens in test_Text: 0.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.3003311]\n",
      "The user is negative about:  [['right', 'ok', 'yeah']]\n",
      "Energy threshold - after:  400\n",
      "that was a high tackle\n",
      "Unknown tokens in test_Text: 20.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.19791813]\n",
      "The user is negative about:  [['tackle']]\n",
      "Energy threshold - after:  400\n",
      "I love deep learning\n",
      "Unknown tokens in test_Text: 0.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.9899657]\n",
      "The User is positive about:  [['love', 'deep', 'learning']]\n",
      "Energy threshold - after:  400\n",
      "Could not understand audio\n",
      "Energy threshold - after:  400\n",
      "I don't hate machine learning\n",
      "Unknown tokens in test_Text: 20.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.04762758]\n",
      "The user is negative about:  [['t', 'machine', 'learning', 'don', 'hate']]\n",
      "Energy threshold - after:  400\n",
      "Could not understand audio\n",
      "Energy threshold - after:  400\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print (device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# import gensim\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from gensim import corpora, models\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# # create English stop words list\n",
    "# en_stop = get_stop_words('en')\n",
    "\n",
    "# # Create p_stemmer of class PorterStemmer\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# #Loop should start-----------------------------------------------------------------------------\n",
    "\n",
    "# while True:\n",
    "#     speak(\"Say something!\")\n",
    "#     resultAudio = listen()\n",
    "    \n",
    "#     if resultAudio!=\"\":\n",
    "#         speak(\"I heard you say \" + resultAudio)\n",
    "\n",
    "#     ####################### Preprocessing for sentiment analysis starts here---------------------------------------    \n",
    "\n",
    "#         testwords = resultAudio.split()\n",
    "\n",
    "\n",
    "#         def createtestMatrix(sentence, word2Idx):\n",
    "#             unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "#             paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "\n",
    "\n",
    "#             testMatrix = []\n",
    "#             unknownWordCount = 0\n",
    "#             wordCount = 0\n",
    "\n",
    "#             targetWordIdx = 0\n",
    "\n",
    "#             for word in sentence:\n",
    "#                 wordCount += 1\n",
    "\n",
    "#                 if word in word2Idx:\n",
    "#                     wordIdx = word2Idx[word]\n",
    "#                 elif word.lower() in word2Idx:\n",
    "#                     wordIdx = word2Idx[word.lower()]\n",
    "#                 else:\n",
    "#                     wordIdx = unknownIdx\n",
    "#                     unknownWordCount += 1\n",
    "#                 testMatrix.append(wordIdx)\n",
    "#             print(\"Unknown tokens in test_Text: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "\n",
    "#             return testMatrix\n",
    "\n",
    "\n",
    "#         word2Idx_path = open(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/word2Idx.pickle\", \"rb\")\n",
    "#         word2Idx = pkl.load(word2Idx_path)\n",
    "\n",
    "#         if len(testwords)!=0:\n",
    "#             finalTest_matrix = createtestMatrix(testwords, word2Idx)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         resultFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/resultdata.pkl.gz'\n",
    "\n",
    "#         testarray = np.array(finalTest_matrix)\n",
    "#         testarray = testarray.reshape(1,len(finalTest_matrix))\n",
    "\n",
    "#         testarray.tolist()\n",
    "#         testarray.shape\n",
    "\n",
    "#         f = gzip.open(resultFilePath, 'wb')\n",
    "#         pkl.dump(testarray, f)\n",
    "#         f.close()\n",
    "\n",
    "#     ##########################pre-processing for the sentiment analysis ends here\n",
    "\n",
    "\n",
    "#     ##########################Preprocessing for topic modelling starts here\n",
    "\n",
    "#         # compile sample documents into a list\n",
    "#         doc_set = [resultAudio]\n",
    "\n",
    "#         # list for tokenized documents in loop\n",
    "#         texts = []\n",
    "\n",
    "#         # loop through document list\n",
    "#         for i in doc_set:\n",
    "\n",
    "#             # clean and tokenize document string\n",
    "#             raw = i.lower()\n",
    "#             tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "#             # remove stop words from tokens\n",
    "#             stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "#             # stem tokens\n",
    "#     #         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "#             # add tokens to list\n",
    "#             texts.append(stopped_tokens)\n",
    "\n",
    "\n",
    "#         # turn our tokenized documents into a id <-> term dictionary\n",
    "#         dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#         # convert tokenized documents into a document-term matrix\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # generate LDA model\n",
    "#         ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "#         topicresult = ldamodel.print_topics(num_topics=1, num_words=5)\n",
    "#         topicList = []\n",
    "\n",
    "#         for j in range(len(topicresult)):\n",
    "#             topic = topicresult[j][1]\n",
    "#         # topic = topic.strip('\\\"')\n",
    "#             x = topic.split(\"+\")\n",
    "#             a = []\n",
    "#             words=[]\n",
    "\n",
    "#             for i in range(len(x)):\n",
    "#                 b = x[i][6:].strip(' ')\n",
    "#                 b = b.strip('*')\n",
    "#                 b = b.strip('\"')\n",
    "#                 words.append(b)\n",
    "#                 pos = nltk.pos_tag(words)\n",
    "#     #         print(b)\n",
    "#                 if pos[0][1][0]=='N' or pos[0][1][0]=='V':\n",
    "#                     a.append(b)\n",
    "#                 words = []\n",
    "#                 pos=[]\n",
    "#         #topicList will have all the topics if the num_topics>1        \n",
    "#         topicList.append(a)\n",
    "\n",
    "#         str1 =  \" \".join(stri for stri in a)\n",
    "\n",
    "#     ##########################preprocessing for topic modelling ends here\n",
    "\n",
    "\n",
    "#     ############################Deeplearning model:\n",
    "\n",
    "#         from keras.models import load_model\n",
    "#         from keras.preprocessing import sequence\n",
    "\n",
    "#         loadedModel = load_model(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/models/model1.h5\")\n",
    "\n",
    "#         result_data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "#         print(\"input data loaded!\")\n",
    "\n",
    "#         result_data.tolist()\n",
    "\n",
    "\n",
    "#         data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/data.pkl.gz\",\"rb\"))\n",
    "#         print(\"data loaded!\")\n",
    "\n",
    "#         train_labels = data['train']['labels']\n",
    "#         train_sentences = data['train']['sentences']\n",
    "\n",
    "#         dev_labels = data['dev']['labels']\n",
    "#         dev_sentences = data['dev']['sentences']\n",
    "\n",
    "#         test_labels = data['test']['labels']\n",
    "#         test_sentences = data['test']['sentences']\n",
    "\n",
    "#         word_embeddings = data['wordEmbeddings']\n",
    "\n",
    "\n",
    "#         # :: Find the longest sentence in our dataset ::\n",
    "#         max_sentence_len = 0\n",
    "#         for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "#             max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "#         print(\"Longest sentence: %d\" % max_sentence_len)\n",
    "\n",
    "#         batch_size = 50\n",
    "#         result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "\n",
    "#         result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)\n",
    "\n",
    "#         senti = result_y_pred.ravel()\n",
    "#         print (senti)\n",
    "\n",
    "#         if(senti[0]>=0.65):\n",
    "#             speak(\"The User is positive\")\n",
    "#             print(\"The User is positive about: \", topicList)\n",
    "#         elif(senti[0]>=0.35 and senti[0]<0.65):\n",
    "#             speak(\"The user is Neutral\")\n",
    "#             print(\"The user is Neutral about: \", topicList)\n",
    "#         else:\n",
    "#             speak(\"The user is negative\")\n",
    "#             print(\"The user is negative about: \", topicList)\n",
    "        \n",
    "#     else:\n",
    "#         speak(\"Sorry I could not understand that!\")\n",
    "\n",
    "# #Loop should end here----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens in test_Text: 7.14%\n"
     ]
    }
   ],
   "source": [
    "#--------------------excluding the speech recognition and text to speech concept from the previous implementation:\n",
    "\n",
    "\n",
    "####################### Preprocessing for sentiment analysis starts here---------------------------------------    \n",
    "\n",
    "resultAudio = 'Dear So Energy Team On my latest bill there is a mention that I could save money by changingfrom the Gorilla to the Kangaroo tariff but I cant see how I go about doingthis.   Can you advise please. Many thanks  Jane Inman'\n",
    "\n",
    "#1. However when my contract finishes on the 2/07/2018 I am moving to a contract which is substantially more expensive than my current deal therefore I prefer to switch to rates that are lower\n",
    "\n",
    "testwords = resultAudio.split()\n",
    "\n",
    "\n",
    "def createtestMatrix(sentence, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "\n",
    "\n",
    "    testMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "\n",
    "    targetWordIdx = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        wordCount += 1\n",
    "\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = unknownIdx\n",
    "            unknownWordCount += 1\n",
    "        testMatrix.append(wordIdx)\n",
    "    print(\"Unknown tokens in test_Text: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "\n",
    "    return testMatrix\n",
    "\n",
    "\n",
    "word2Idx_path = open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/word2Idx.pickle\", \"rb\")\n",
    "word2Idx = pkl.load(word2Idx_path)\n",
    "\n",
    "# if len(testwords)!=0:\n",
    "#     finalTest_matrix = createtestMatrix(testwords, word2Idx)\n",
    "# else:\n",
    "#     continue\n",
    "\n",
    "finalTest_matrix = createtestMatrix(testwords, word2Idx)\n",
    "\n",
    "resultFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/resultdata.pkl.gz'\n",
    "\n",
    "testarray = np.array(finalTest_matrix)\n",
    "testarray = testarray.reshape(1,len(finalTest_matrix))\n",
    "\n",
    "testarray.tolist()\n",
    "testarray.shape\n",
    "\n",
    "f = gzip.open(resultFilePath, 'wb')\n",
    "pkl.dump(testarray, f)\n",
    "f.close()\n",
    "\n",
    "##########################pre-processing for the sentiment analysis ends here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################Preprocessing for topic modelling starts here\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [resultAudio]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "    # stem tokens\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    # add tokens to list\n",
    "    texts.append(stopped_tokens)\n",
    "\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "topicresult = ldamodel.print_topics(num_topics=10, num_words=5)\n",
    "topicList = []\n",
    "\n",
    "for j in range(len(topicresult)):\n",
    "    topic = topicresult[j][1]\n",
    "# topic = topic.strip('\\\"')\n",
    "    x = topic.split(\"+\")\n",
    "    a = []\n",
    "    words=[]\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        b = x[i][6:].strip(' ')\n",
    "        b = b.strip('*')\n",
    "        b = b.strip('\"')\n",
    "        words.append(b)\n",
    "        pos = nltk.pos_tag(words)\n",
    "#         print(b)\n",
    "        if pos[0][1][0]=='N' or pos[0][1][0]=='V':\n",
    "            a.append(b)\n",
    "        words = []\n",
    "        pos=[]\n",
    "#topicList will have all the topics if the num_topics>1        \n",
    "topicList.append(a)\n",
    "\n",
    "str1 =  \" \".join(stri for stri in a)\n",
    "\n",
    "##########################preprocessing for topic modelling ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 704\n",
      "[[0.08725893 0.05795438 0.00773239 0.00435072 0.8427035 ]]\n"
     ]
    }
   ],
   "source": [
    "############################Deeplearning model:\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "loadedModel = load_model(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/soemailModels/classMailModel1.h5\")\n",
    "\n",
    "result_data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "print(\"input data loaded!\")\n",
    "\n",
    "result_data.tolist()\n",
    "\n",
    "\n",
    "data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/maildata.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")\n",
    "\n",
    "train_labels = data['train']['labels']\n",
    "train_sentences = data['train']['sentences']\n",
    "\n",
    "dev_labels = data['dev']['labels']\n",
    "dev_sentences = data['dev']['sentences']\n",
    "\n",
    "test_labels = data['test']['labels']\n",
    "test_sentences = data['test']['sentences']\n",
    "\n",
    "word_embeddings = data['wordEmbeddings']\n",
    "\n",
    "\n",
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "print(\"Longest sentence: %d\" % max_sentence_len)\n",
    "\n",
    "batch_size = 50\n",
    "result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "\n",
    "result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)\n",
    "\n",
    "# senti = result_y_pred.ravel()\n",
    "# print (senti)\n",
    "\n",
    "print(result_y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['money', 'mention', 'tariff', 'dear']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y_pred = result_y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08725893497467041,\n",
       " 0.05795437842607498,\n",
       " 0.00773239228874445,\n",
       " 0.004350717179477215,\n",
       " 0.8427035212516785]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result_y_pred[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.8427035212516785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "index, value = max(enumerate(result), key=operator.itemgetter(1))\n",
    "print(index,value)\n",
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user Rating is Terrible  [['money', 'mention', 'tariff', 'dear']]\n"
     ]
    }
   ],
   "source": [
    "if(index==0):\n",
    "    print(\"The User Rating is Excellent! \", topicList)\n",
    "elif(index==1):\n",
    "    print(\"The user Rating is Good! \", topicList)\n",
    "elif(index==2):\n",
    "    print(\"The user Rating is Average! \", topicList)\n",
    "elif(index==3):\n",
    "    print(\"The user Rating is Bad! \", topicList)\n",
    "elif(index==4):\n",
    "    print(\"The user Rating is Terrible \", topicList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
