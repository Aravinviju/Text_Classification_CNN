{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We download English word embeddings from here https://www.cs.york.ac.uk/nlp/extvec/\n",
    "embeddingsPath = '/home/dl1/Arav/Sentence_classic/code/embeddings/wiki_extvec.gz'\n",
    "\n",
    "#Train, Dev, and Test files\n",
    "folder = '/home/dl1/Arav/Sentence_classic/code/data/'\n",
    "files = [folder+'train.txt',  folder+'dev.txt', folder+'test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    xMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        targetWordIdx = 0\n",
    "        \n",
    "        sentenceWordIdx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            wordCount += 1\n",
    "            \n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "                \n",
    "            sentenceWordIdx.append(wordIdx)\n",
    "            \n",
    "        xMatrix.append(sentenceWordIdx)\n",
    "       \n",
    "    \n",
    "    print(\"Unknown tokens: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "    return xMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []    \n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath):   \n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dl1/Arav/Sentence_classic/code/data/train.txt 5330 sentences\n",
      "/home/dl1/Arav/Sentence_classic/code/data/dev.txt 2664 sentences\n",
      "/home/dl1/Arav/Sentence_classic/code/data/test.txt 2668 sentences\n"
     ]
    }
   ],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "#      Start of the preprocessing\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "\n",
    "outputFilePath = '/home/dl1/Arav/Sentence_classic/code/pkl/data.pkl.gz'\n",
    "\n",
    "\n",
    "trainDataset = readFile(files[0])\n",
    "devDataset = readFile(files[1])\n",
    "testDataset = readFile(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Compute which words are needed for the train/dev/test set ::\n",
    "words = {}\n",
    "for sentences, labels in [trainDataset, devDataset, testDataset]:       \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            words[token.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Downloads the embeddings from the York webserver ::\n",
    "if not os.path.isfile(embeddingsPath):\n",
    "    basename = os.path.basename(embeddingsPath)\n",
    "    if basename == 'wiki_extvec.gz':\n",
    "\t       print(\"Start downloading word embeddings for English using wget ...\")\n",
    "\t       #os.system(\"wget https://www.cs.york.ac.uk/nlp/extvec/\"+basename+\" -P embeddings/\") #Original path from York University\n",
    "\t       os.system(\"wget https://public.ukp.informatik.tu-darmstadt.de/reimers/2017_english_embeddings/\"+basename+\" -P embeddings/\")\n",
    "    else:\n",
    "        print(embeddingsPath, \"does not exist. Please provide pre-trained embeddings\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Load the pre-trained embeddings file ::\n",
    "fEmbeddings = gzip.open(embeddingsPath, \"r\") if embeddingsPath.endswith('.gz') else open(embeddingsPath, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained embeddings file\n",
      "Embeddings shape:  (16554, 300)\n",
      "Len words:  21347\n"
     ]
    }
   ],
   "source": [
    "print(\"Load pre-trained embeddings file\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector for 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if word.lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)\n",
    "       \n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "print(\"Embeddings shape: \", wordEmbeddings.shape)\n",
    "print(\"Len words: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx_path = open(\"/home/dl1/Arav/Sentence_classic/code/pkl/word2Idx.pickle\",\"wb\")\n",
    "pkl.dump(word2Idx,word2Idx_path)\n",
    "word2Idx_path.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordEmbeddings\n",
    "# word2Idx\n",
    "# trainDataset[1]\n",
    "# word2Idx['UNKNOWN_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # :: Create matrices ::\n",
    "# train_matrix = createMatrices(trainDataset[0], word2Idx)\n",
    "# dev_matrix = createMatrices(devDataset[0], word2Idx)\n",
    "# test_matrix = createMatrices(testDataset[0], word2Idx)\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     'wordEmbeddings': wordEmbeddings, 'word2Idx': word2Idx,\n",
    "#     'train': {'sentences': train_matrix, 'labels': trainDataset[1]},\n",
    "#     'dev':   {'sentences': dev_matrix, 'labels': devDataset[1]},\n",
    "#     'test':  {'sentences': test_matrix, 'labels': testDataset[1]}\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "###check\n",
    "# print(test_matrix)\n",
    "# testDataset[1]\n",
    "# trainDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"The greatest pleasure in life is doing what people say you cannot do.\"\n",
    "# text = \"She was dying to say something sarcastic to him, but bit her tongue and stayed silent\"\n",
    "# text = \"I work 40 hours a week to be this poor\"\n",
    "# text = \"I’m busy now. Can I ignore you some other time?\"\n",
    "# text = \"I’m glad to see you’re not letting your education get in the way of your ignorance.\"\n",
    "# text = \"I just won the lottery. The worst part is that I can’t tell my family and friends because if I did, they’d all want some of the money.\"\n",
    "# text = \"I just won the lottery and I am happy about it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"I’d rather not own a highly successful restaurant. Sure, you could be famous and make a ton of money, but just think of those long hours and dealing with the public all the time.\"\n",
    "# text = \"You may say you want a cool sporty car, but I can’t imagine paying that car insurance and getting pulled over by cops all the time.\"\n",
    "# text = \"Dogs could be great companions and really brighten things up if they didn’t make such a mess and all that noise.\"\n",
    "# text = \"My job pays really well and fast tracks its employees into higher positions since they’re growing so quickly, but sitting in a grey, boring cubicle listening to sad FM radio all afternoon from the cubicle next door is just too much. \"\n",
    "# text = \"After getting fired, Johnny and his wife celebrated for the job he would soon have.\"\n",
    "# text = \"Work was cancelled due to inclement weather. An optimistic person enjoyed the time off.\"\n",
    "# text = \"I love my job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Whose car do you think is faster, yours or mine? I have no idea. Well, okay, what kind of car do you drive? A 1988 Honda Accord. That car is a big piece of shit! That is what I can afford. So, it is still a big piece of shit. Well, it works. Works like a big piece of shit works. What does that even mean?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Recognition -> Text -> Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/dl1/anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install gTTS\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for getting a audio source file for further testing the speech to text using pyttsx: \n",
    "from gtts import gTTS\n",
    "import wave\n",
    "# import scipy.io.wavfile\n",
    "blabla = (\"well it's about time you got here\")\n",
    "tts = gTTS(text=blabla, lang='en')\n",
    "# data2 = np.asarray(tts, dtype=np.int16)\n",
    "tts.save(\"/home/dl1/Arav/Sentence_classic/code/newtest.wav\")\n",
    "# wave.open(\"/home/dl1/Arav/Sentence_classic/code/nwtest.wav\",mode='wb')\n",
    "# scipy.io.wavfile.write(\"/home/dl1/Arav/Sentence_classic/code/newtest.wav\",48000,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#speech to text -> input text:\n",
    "\n",
    "import speech_recognition\n",
    "import pyttsx3\n",
    "\n",
    "speech_engine = pyttsx3.init('espeak') # see http://pyttsx.readthedocs.org/en/latest/engine.html#pyttsx.init\n",
    "print(speech_engine.getProperty('rate'))\n",
    "speech_engine.setProperty('rate', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    speech_engine.say(text)\n",
    "    speech_engine.runAndWait()\n",
    "\n",
    "recognizer = speech_recognition.Recognizer()\n",
    "\n",
    "def listen():\n",
    "#     with speech_recognition.AudioFile(\"/home/dl1/Arav/Sentence_classic/code/newtest.wav\") as source:\n",
    "    with speech_recognition.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "        print(recognizer.energy_threshold)\n",
    "        print(\"Chunking rate:\", source.CHUNK)\n",
    "        print(\"format rate:\",source.format)\n",
    "#         recognizer.energy_threshold +=80\n",
    "#         print(recognizer.energy_threshold)\n",
    "\n",
    "        audio = recognizer.listen(source)\n",
    "        \n",
    "#         audio = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio, language='en-GB')\n",
    "        print(text)\n",
    "        return text\n",
    "    \n",
    "#         or:return recognizer.recognize_sphinx(audio)\n",
    "        \n",
    "    except speech_recognition.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "#         reply = \"sorry, I could not understand that\"\n",
    "#         return reply\n",
    "    except speech_recognition.RequestError as e:\n",
    "        print(\"Recog Error; {0}\".format(e))\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_recognition.AudioFile\n",
    "# recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"Say something!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.59210593546467\n",
      "Chunking rate: 1024\n",
      "format rate: 8\n",
      "hello hello hello hello hello\n"
     ]
    }
   ],
   "source": [
    "resultAudio = listen()\n",
    "speak(\"I heard you say \" + resultAudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'hello', 'hello', 'hello', 'hello']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testwords = resultAudio.split()\n",
    "testwords\n",
    "# sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createtestMatrix(sentence, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    testMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "    targetWordIdx = 0\n",
    "\n",
    "#     sentenceWordIdx = []\n",
    "\n",
    "    for word in sentence:\n",
    "        wordCount += 1\n",
    "\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = unknownIdx\n",
    "            unknownWordCount += 1\n",
    "\n",
    "#         sentenceWordIdx.append(wordIdx)\n",
    "\n",
    "        testMatrix.append(wordIdx)\n",
    "\n",
    "    \n",
    "    print(\"Unknown tokens in test_Text: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "\n",
    "    return testMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx_path = open(\"/home/dl1/Arav/Sentence_classic/code/pkl/word2Idx.pickle\", \"rb\")\n",
    "word2Idx = pkl.load(word2Idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens in test_Text: 100.00%\n"
     ]
    }
   ],
   "source": [
    "finalTest_matrix = createtestMatrix(testwords, word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTest_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultFilePath = '/home/dl1/Arav/Sentence_classic/code/pkl/resultdata.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testarray = np.array(finalTest_matrix)\n",
    "testarray = testarray.reshape(1,len(finalTest_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testarray.tolist()\n",
    "testarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(resultFilePath, 'wb')\n",
    "pkl.dump(testarray, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning model prediction on sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl1/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.2765223], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "loadedModel = load_model(\"/home/dl1/Arav/Sentence_classic/models/model1.h5\")\n",
    "\n",
    "result_data = pkl.load(gzip.open(\"/home/dl1/Arav/Sentence_classic/code/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")\n",
    "\n",
    "result_data.tolist()\n",
    "\n",
    "\n",
    "data = pkl.load(gzip.open(\"/home/dl1/Arav/Sentence_classic/code/pkl/data.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")\n",
    "\n",
    "train_labels = data['train']['labels']\n",
    "train_sentences = data['train']['sentences']\n",
    "\n",
    "dev_labels = data['dev']['labels']\n",
    "dev_sentences = data['dev']['sentences']\n",
    "\n",
    "test_labels = data['test']['labels']\n",
    "test_sentences = data['test']['sentences']\n",
    "\n",
    "word_embeddings = data['wordEmbeddings']\n",
    "\n",
    "\n",
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "print(\"Longest sentence: %d\" % max_sentence_len)\n",
    "\n",
    "batch_size = 50\n",
    "result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "\n",
    "result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)\n",
    "\n",
    "senti = result_y_pred.ravel()\n",
    "\n",
    "senti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning model's prediction (speech):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is negative\n"
     ]
    }
   ],
   "source": [
    "if(senti[0]>=0.65):\n",
    "    speak(\"The User is positive\")\n",
    "    print(\"The User is positive\")\n",
    "elif(senti[0]>0.35 and senti[0]<0.65):\n",
    "    speak(\"The user is Neutral\")\n",
    "    print(\"The user is Neutral\")\n",
    "else:\n",
    "    speak(\"The user is negative\")\n",
    "    print(\"The user is negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-existing LDA Service (Algorithmia) Usage  starts from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5562b4f337f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtextLDA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtextLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtextLDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "textLDA=[]\n",
    "textLDA.append(text)\n",
    "textLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usage of the LDA - Algorithmia\n",
    "\n",
    "# !sudo pip install algorithmia\n",
    "import Algorithmia\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doing',\n",
       " 'too',\n",
       " 'more',\n",
       " \"doesn't\",\n",
       " 'aren',\n",
       " \"mightn't\",\n",
       " 'can',\n",
       " 't',\n",
       " 'didn',\n",
       " \"she's\",\n",
       " 'an',\n",
       " \"that'll\",\n",
       " 'hers',\n",
       " 'than',\n",
       " 'my',\n",
       " 'further',\n",
       " 'his',\n",
       " 'she',\n",
       " 'only',\n",
       " 'yourselves',\n",
       " 'does',\n",
       " 'our',\n",
       " 'it',\n",
       " \"couldn't\",\n",
       " 'down',\n",
       " \"you'd\",\n",
       " 'for',\n",
       " 'a',\n",
       " 'and',\n",
       " 'after',\n",
       " 'm',\n",
       " 'your',\n",
       " \"shouldn't\",\n",
       " 'there',\n",
       " \"you've\",\n",
       " 'until',\n",
       " 'which',\n",
       " 'these',\n",
       " \"it's\",\n",
       " 'were',\n",
       " 'the',\n",
       " 'own',\n",
       " 'while',\n",
       " 'wouldn',\n",
       " 'do',\n",
       " 'most',\n",
       " 'very',\n",
       " 'ma',\n",
       " 'mustn',\n",
       " 'at',\n",
       " 'isn',\n",
       " 'we',\n",
       " 'you',\n",
       " 'what',\n",
       " 'should',\n",
       " 're',\n",
       " 'ours',\n",
       " 'no',\n",
       " 'under',\n",
       " 'from',\n",
       " 'wasn',\n",
       " 'in',\n",
       " \"aren't\",\n",
       " 'over',\n",
       " 'all',\n",
       " \"shan't\",\n",
       " 'nor',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'hadn',\n",
       " 'yours',\n",
       " 'just',\n",
       " 'them',\n",
       " 'will',\n",
       " 'both',\n",
       " 'their',\n",
       " 'y',\n",
       " 'doesn',\n",
       " 'o',\n",
       " 'themselves',\n",
       " 'during',\n",
       " 'ain',\n",
       " \"didn't\",\n",
       " 'not',\n",
       " 'by',\n",
       " 'when',\n",
       " 's',\n",
       " 'he',\n",
       " 'out',\n",
       " \"wouldn't\",\n",
       " 'few',\n",
       " 'had',\n",
       " 'don',\n",
       " 'be',\n",
       " 'into',\n",
       " 'shan',\n",
       " 'each',\n",
       " 'herself',\n",
       " 'has',\n",
       " 'theirs',\n",
       " 'i',\n",
       " 'about',\n",
       " 'to',\n",
       " 'some',\n",
       " 'that',\n",
       " 'whom',\n",
       " 'of',\n",
       " 'below',\n",
       " 'other',\n",
       " \"should've\",\n",
       " 'haven',\n",
       " 'needn',\n",
       " 'why',\n",
       " \"hasn't\",\n",
       " 'him',\n",
       " \"isn't\",\n",
       " 'because',\n",
       " 'being',\n",
       " 'here',\n",
       " 'll',\n",
       " 'they',\n",
       " 'is',\n",
       " 'did',\n",
       " 'same',\n",
       " \"you're\",\n",
       " 'have',\n",
       " 'this',\n",
       " 'those',\n",
       " 'having',\n",
       " 'how',\n",
       " 'couldn',\n",
       " \"weren't\",\n",
       " \"won't\",\n",
       " 'won',\n",
       " 'are',\n",
       " 'hasn',\n",
       " 'himself',\n",
       " 'd',\n",
       " 'shouldn',\n",
       " 'between',\n",
       " \"mustn't\",\n",
       " 'been',\n",
       " 'any',\n",
       " 'against',\n",
       " 'again',\n",
       " 'then',\n",
       " \"don't\",\n",
       " 'ourselves',\n",
       " \"needn't\",\n",
       " \"hadn't\",\n",
       " 'but',\n",
       " 'if',\n",
       " 'yourself',\n",
       " 'its',\n",
       " 'through',\n",
       " 'who',\n",
       " 'am',\n",
       " 've',\n",
       " 'before',\n",
       " 'weren',\n",
       " 'mightn',\n",
       " 'such',\n",
       " 'where',\n",
       " \"wasn't\",\n",
       " 'or',\n",
       " 'me',\n",
       " \"haven't\",\n",
       " 'above',\n",
       " 'up',\n",
       " 'so',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'myself',\n",
       " 'as',\n",
       " \"you'll\",\n",
       " 'her',\n",
       " 'now',\n",
       " 'was']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stopwords = [w for w in stop_words]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'afford': 1, 'honda': 1, 'piece': 3, 'works': 3}, {'accord': 1, 'big': 3, 'car': 3, 'shit': 3}]\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"docsList\": textLDA,\n",
    "#     \"mode\": \"quality\",\n",
    "    \"customSettings\":{\n",
    "        \"numTopics\":2,\n",
    "        \"numIterations\":50,\n",
    "        \"numWords\":4\n",
    "    },\n",
    "    \"stopWordsList\":stopwords\n",
    "}\n",
    "client = Algorithmia.client('sim+KZtb16R1rtOXC0dk9Y4sqEb1')\n",
    "algo = client.algo('nlp/LDA/1.0.0')\n",
    "LDAresult = algo.pipe(input).result\n",
    "print(LDAresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'afford': 1, 'honda': 1, 'piece': 3, 'works': 3}\n",
      "{'accord': 1, 'big': 3, 'car': 3, 'shit': 3}\n"
     ]
    }
   ],
   "source": [
    "# jsontopython = json.load(LDAresult)\n",
    "# print(jsontopython)\n",
    "for item in LDAresult:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of the pre-existing sentiment analysis services from Algorithmia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'compound': 0.836, 'negative': 0, 'neutral': 0.582, 'positive': 0.418, 'sentence': 'The greatest pleasure in life is doing what people say you cannot do.'}]\n"
     ]
    }
   ],
   "source": [
    "#Algorithmia sentiment module: (positive,negative,neutral/compound)\n",
    "# import Algorithmia\n",
    "\n",
    "input = {\n",
    "  \"sentence\": \"The greatest pleasure in life is doing what people say you cannot do.\"\n",
    "}\n",
    "client = Algorithmia.client('sim+KZtb16R1rtOXC0dk9Y4sqEb1')\n",
    "algo = client.algo('nlp/SocialSentimentAnalysis/0.1.4')\n",
    "print(algo.pipe(input).result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'document': 'The greatest pleasure in life is doing what people say you cannot do.', 'sentiment': 0.836}]\n"
     ]
    }
   ],
   "source": [
    "#Algorithmia sentiment module: (positive,negative) - Sentiment value between -1 and 1 (very negative to very positive)\n",
    "# import Algorithmia\n",
    "\n",
    "input = {\n",
    "  \"document\": \"The greatest pleasure in life is doing what people say you cannot do.\"\n",
    "}\n",
    "client = Algorithmia.client('sim+KZtb16R1rtOXC0dk9Y4sqEb1')\n",
    "algo = client.algo('nlp/SentimentAnalysis/1.0.4')\n",
    "print(algo.pipe(input).result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microphone with name \"HDA Intel PCH: ALC1220 Analog (hw:0,0)\" found for `Microphone(device_index=0)`\n",
      "Microphone with name \"HDA Intel PCH: ALC1220 Alt Analog (hw:0,2)\" found for `Microphone(device_index=1)`\n",
      "Microphone with name \"HDA NVidia: HDMI 0 (hw:1,3)\" found for `Microphone(device_index=2)`\n",
      "Microphone with name \"HDA NVidia: HDMI 1 (hw:1,7)\" found for `Microphone(device_index=3)`\n",
      "Microphone with name \"HDA NVidia: HDMI 2 (hw:1,8)\" found for `Microphone(device_index=4)`\n",
      "Microphone with name \"HDA NVidia: HDMI 3 (hw:1,9)\" found for `Microphone(device_index=5)`\n",
      "Microphone with name \"HDA NVidia: HDMI 0 (hw:2,3)\" found for `Microphone(device_index=6)`\n",
      "Microphone with name \"HDA NVidia: HDMI 1 (hw:2,7)\" found for `Microphone(device_index=7)`\n",
      "Microphone with name \"HDA NVidia: HDMI 2 (hw:2,8)\" found for `Microphone(device_index=8)`\n",
      "Microphone with name \"HDA NVidia: HDMI 3 (hw:2,9)\" found for `Microphone(device_index=9)`\n",
      "Microphone with name \"HDA NVidia: HDMI 0 (hw:3,3)\" found for `Microphone(device_index=10)`\n",
      "Microphone with name \"HDA NVidia: HDMI 1 (hw:3,7)\" found for `Microphone(device_index=11)`\n",
      "Microphone with name \"HDA NVidia: HDMI 2 (hw:3,8)\" found for `Microphone(device_index=12)`\n",
      "Microphone with name \"HDA NVidia: HDMI 3 (hw:3,9)\" found for `Microphone(device_index=13)`\n",
      "Microphone with name \"HDA NVidia: HDMI 0 (hw:4,3)\" found for `Microphone(device_index=14)`\n",
      "Microphone with name \"HDA NVidia: HDMI 1 (hw:4,7)\" found for `Microphone(device_index=15)`\n",
      "Microphone with name \"HDA NVidia: HDMI 2 (hw:4,8)\" found for `Microphone(device_index=16)`\n",
      "Microphone with name \"HDA NVidia: HDMI 3 (hw:4,9)\" found for `Microphone(device_index=17)`\n",
      "Microphone with name \"sysdefault\" found for `Microphone(device_index=18)`\n",
      "Microphone with name \"front\" found for `Microphone(device_index=19)`\n",
      "Microphone with name \"surround21\" found for `Microphone(device_index=20)`\n",
      "Microphone with name \"surround40\" found for `Microphone(device_index=21)`\n",
      "Microphone with name \"surround41\" found for `Microphone(device_index=22)`\n",
      "Microphone with name \"surround50\" found for `Microphone(device_index=23)`\n",
      "Microphone with name \"surround51\" found for `Microphone(device_index=24)`\n",
      "Microphone with name \"surround71\" found for `Microphone(device_index=25)`\n",
      "Microphone with name \"pulse\" found for `Microphone(device_index=26)`\n",
      "Microphone with name \"dmix\" found for `Microphone(device_index=27)`\n",
      "Microphone with name \"default\" found for `Microphone(device_index=28)`\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "    print(\"Microphone with name \\\"{1}\\\" found for `Microphone(device_index={0})`\".format(index, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
