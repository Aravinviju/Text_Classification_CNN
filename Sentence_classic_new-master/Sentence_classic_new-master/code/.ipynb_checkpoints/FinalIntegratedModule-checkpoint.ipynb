{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/mailtrain.txt 8094 sentences\n",
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/maildev.txt 4047 sentences\n",
      "C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/mailtest.txt 4048 sentences\n",
      "Load pre-trained embeddings file\n",
      "Embeddings shape:  (7290, 300)\n",
      "Len words:  11720\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl\n",
    "    \n",
    "\n",
    "#We download English word embeddings from here https://www.cs.york.ac.uk/nlp/extvec/\n",
    "embeddingsPath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/embeddings/wiki_extvec.gz'\n",
    "\n",
    "#Train, Dev, and Test files\n",
    "folder = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/'\n",
    "files = [folder+'mailtrain.txt',  folder+'maildev.txt', folder+'mailtest.txt']\n",
    "\n",
    "\n",
    "def createMatrices(sentences, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    xMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        targetWordIdx = 0\n",
    "        \n",
    "        sentenceWordIdx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            wordCount += 1\n",
    "            \n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "                \n",
    "            sentenceWordIdx.append(wordIdx)\n",
    "            \n",
    "        xMatrix.append(sentenceWordIdx)\n",
    "       \n",
    "    \n",
    "    print(\"Unknown tokens: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "    return xMatrix\n",
    "\n",
    "def readFile(filepath):\n",
    "    sentences = []    \n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath, encoding = \"utf-8\"):   \n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "#      Start of the preprocessing\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "\n",
    "outputFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/maildata.pkl.gz'\n",
    "\n",
    "\n",
    "trainDataset = readFile(files[0])\n",
    "devDataset = readFile(files[1])\n",
    "testDataset = readFile(files[2])\n",
    "\n",
    "\n",
    "# :: Compute which words are needed for the train/dev/test set ::\n",
    "words = {}\n",
    "for sentences, labels in [trainDataset, devDataset, testDataset]:       \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            words[token.lower()] = True\n",
    "\n",
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "# :: Downloads the embeddings from the York webserver ::\n",
    "if not os.path.isfile(embeddingsPath):\n",
    "    basename = os.path.basename(embeddingsPath)\n",
    "    if basename == 'wiki_extvec.gz':\n",
    "\t       print(\"Start downloading word embeddings for English using wget ...\")\n",
    "\t       #os.system(\"wget https://www.cs.york.ac.uk/nlp/extvec/\"+basename+\" -P embeddings/\") #Original path from York University\n",
    "\t       os.system(\"wget https://public.ukp.informatik.tu-darmstadt.de/reimers/2017_english_embeddings/\"+basename+\" -P embeddings/\")\n",
    "    else:\n",
    "        print(embeddingsPath, \"does not exist. Please provide pre-trained embeddings\")\n",
    "        exit()\n",
    "        \n",
    "\n",
    "# :: Load the pre-trained embeddings file ::\n",
    "fEmbeddings = gzip.open(embeddingsPath, \"r\") if embeddingsPath.endswith('.gz') else open(embeddingsPath, encoding=\"utf8\")\n",
    "\n",
    "\n",
    "print(\"Load pre-trained embeddings file\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector for 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if word.lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)\n",
    "       \n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "print(\"Embeddings shape: \", wordEmbeddings.shape)\n",
    "print(\"Len words: \", len(words))\n",
    "\n",
    "\n",
    "word2Idx_path = open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/word2Idx.pickle\",\"wb\")\n",
    "pkl.dump(word2Idx,word2Idx_path)\n",
    "word2Idx_path.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual speech rate is:  125\n",
      "Energy threshold - before:  300\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------speech to text -> input text:\n",
    "\n",
    "# import speech_recognition\n",
    "# import pyttsx3\n",
    "\n",
    "# speech_engine = pyttsx3.init('espeak') # see http://pyttsx.readthedocs.org/en/latest/engine.html#pyttsx.init\n",
    "# print(\"Actual speech rate is: \",speech_engine.getProperty('rate'))\n",
    "# speech_engine.setProperty('rate', 125)\n",
    "\n",
    "# def speak(text):\n",
    "#     speech_engine.say(text)\n",
    "#     speech_engine.runAndWait()\n",
    "\n",
    "# recognizer = speech_recognition.Recognizer()\n",
    "\n",
    "# print(\"Energy threshold - before: \", recognizer.energy_threshold)\n",
    "\n",
    "\n",
    "\n",
    "# def listen():\n",
    "# #     with speech_recognition.AudioFile(\"/home/dl1/Arav/Sentence_classic/code/newtest.wav\") as source:\n",
    "#     with speech_recognition.Microphone() as source:\n",
    "#         recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "#         recognizer.dynamic_energy_threshold = False\n",
    "# #         print(\"Chunking rate:\", source.CHUNK)\n",
    "# #         print(\"format rate:\", source.format)\n",
    "#         recognizer.energy_threshold =400\n",
    "#         print(\"Energy threshold - after: \", recognizer.energy_threshold)\n",
    "\n",
    "#         audio = recognizer.listen(source)\n",
    "        \n",
    "# #         audio = recognizer.record(source)\n",
    "\n",
    "#     try:\n",
    "#         text = recognizer.recognize_google(audio, language='en-GB')\n",
    "#         print(text)\n",
    "#         return text\n",
    "    \n",
    "# #         or:return recognizer.recognize_sphinx(audio)\n",
    "        \n",
    "#     except speech_recognition.UnknownValueError:\n",
    "#         print(\"Could not understand audio\")\n",
    "# #         reply = \"sorry, I could not understand that\"\n",
    "# #         return reply\n",
    "#     except speech_recognition.RequestError as e:\n",
    "#         print(\"Recog Error; {0}\".format(e))\n",
    "\n",
    "#     return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12311574173532944334\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 170459136\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 10317876230059822873\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 338231296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16976386436290909991\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Energy threshold - after:  400\n",
      "tackle\n",
      "Unknown tokens in test_Text: 100.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.25253412]\n",
      "The user is negative about:  [['tackle']]\n",
      "Energy threshold - after:  400\n",
      "ok right yeah\n",
      "Unknown tokens in test_Text: 0.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.3003311]\n",
      "The user is negative about:  [['right', 'ok', 'yeah']]\n",
      "Energy threshold - after:  400\n",
      "that was a high tackle\n",
      "Unknown tokens in test_Text: 20.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.19791813]\n",
      "The user is negative about:  [['tackle']]\n",
      "Energy threshold - after:  400\n",
      "I love deep learning\n",
      "Unknown tokens in test_Text: 0.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.9899657]\n",
      "The User is positive about:  [['love', 'deep', 'learning']]\n",
      "Energy threshold - after:  400\n",
      "Could not understand audio\n",
      "Energy threshold - after:  400\n",
      "I don't hate machine learning\n",
      "Unknown tokens in test_Text: 20.00%\n",
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 59\n",
      "[0.04762758]\n",
      "The user is negative about:  [['t', 'machine', 'learning', 'don', 'hate']]\n",
      "Energy threshold - after:  400\n",
      "Could not understand audio\n",
      "Energy threshold - after:  400\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print (device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# import gensim\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from gensim import corpora, models\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# # create English stop words list\n",
    "# en_stop = get_stop_words('en')\n",
    "\n",
    "# # Create p_stemmer of class PorterStemmer\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# #Loop should start-----------------------------------------------------------------------------\n",
    "\n",
    "# while True:\n",
    "#     speak(\"Say something!\")\n",
    "#     resultAudio = listen()\n",
    "    \n",
    "#     if resultAudio!=\"\":\n",
    "#         speak(\"I heard you say \" + resultAudio)\n",
    "\n",
    "#     ####################### Preprocessing for sentiment analysis starts here---------------------------------------    \n",
    "\n",
    "#         testwords = resultAudio.split()\n",
    "\n",
    "\n",
    "#         def createtestMatrix(sentence, word2Idx):\n",
    "#             unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "#             paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "\n",
    "\n",
    "#             testMatrix = []\n",
    "#             unknownWordCount = 0\n",
    "#             wordCount = 0\n",
    "\n",
    "#             targetWordIdx = 0\n",
    "\n",
    "#             for word in sentence:\n",
    "#                 wordCount += 1\n",
    "\n",
    "#                 if word in word2Idx:\n",
    "#                     wordIdx = word2Idx[word]\n",
    "#                 elif word.lower() in word2Idx:\n",
    "#                     wordIdx = word2Idx[word.lower()]\n",
    "#                 else:\n",
    "#                     wordIdx = unknownIdx\n",
    "#                     unknownWordCount += 1\n",
    "#                 testMatrix.append(wordIdx)\n",
    "#             print(\"Unknown tokens in test_Text: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "\n",
    "#             return testMatrix\n",
    "\n",
    "\n",
    "#         word2Idx_path = open(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/word2Idx.pickle\", \"rb\")\n",
    "#         word2Idx = pkl.load(word2Idx_path)\n",
    "\n",
    "#         if len(testwords)!=0:\n",
    "#             finalTest_matrix = createtestMatrix(testwords, word2Idx)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         resultFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/resultdata.pkl.gz'\n",
    "\n",
    "#         testarray = np.array(finalTest_matrix)\n",
    "#         testarray = testarray.reshape(1,len(finalTest_matrix))\n",
    "\n",
    "#         testarray.tolist()\n",
    "#         testarray.shape\n",
    "\n",
    "#         f = gzip.open(resultFilePath, 'wb')\n",
    "#         pkl.dump(testarray, f)\n",
    "#         f.close()\n",
    "\n",
    "#     ##########################pre-processing for the sentiment analysis ends here\n",
    "\n",
    "\n",
    "#     ##########################Preprocessing for topic modelling starts here\n",
    "\n",
    "#         # compile sample documents into a list\n",
    "#         doc_set = [resultAudio]\n",
    "\n",
    "#         # list for tokenized documents in loop\n",
    "#         texts = []\n",
    "\n",
    "#         # loop through document list\n",
    "#         for i in doc_set:\n",
    "\n",
    "#             # clean and tokenize document string\n",
    "#             raw = i.lower()\n",
    "#             tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "#             # remove stop words from tokens\n",
    "#             stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "#             # stem tokens\n",
    "#     #         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "#             # add tokens to list\n",
    "#             texts.append(stopped_tokens)\n",
    "\n",
    "\n",
    "#         # turn our tokenized documents into a id <-> term dictionary\n",
    "#         dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#         # convert tokenized documents into a document-term matrix\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # generate LDA model\n",
    "#         ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "#         topicresult = ldamodel.print_topics(num_topics=1, num_words=5)\n",
    "#         topicList = []\n",
    "\n",
    "#         for j in range(len(topicresult)):\n",
    "#             topic = topicresult[j][1]\n",
    "#         # topic = topic.strip('\\\"')\n",
    "#             x = topic.split(\"+\")\n",
    "#             a = []\n",
    "#             words=[]\n",
    "\n",
    "#             for i in range(len(x)):\n",
    "#                 b = x[i][6:].strip(' ')\n",
    "#                 b = b.strip('*')\n",
    "#                 b = b.strip('\"')\n",
    "#                 words.append(b)\n",
    "#                 pos = nltk.pos_tag(words)\n",
    "#     #         print(b)\n",
    "#                 if pos[0][1][0]=='N' or pos[0][1][0]=='V':\n",
    "#                     a.append(b)\n",
    "#                 words = []\n",
    "#                 pos=[]\n",
    "#         #topicList will have all the topics if the num_topics>1        \n",
    "#         topicList.append(a)\n",
    "\n",
    "#         str1 =  \" \".join(stri for stri in a)\n",
    "\n",
    "#     ##########################preprocessing for topic modelling ends here\n",
    "\n",
    "\n",
    "#     ############################Deeplearning model:\n",
    "\n",
    "#         from keras.models import load_model\n",
    "#         from keras.preprocessing import sequence\n",
    "\n",
    "#         loadedModel = load_model(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/models/model1.h5\")\n",
    "\n",
    "#         result_data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "#         print(\"input data loaded!\")\n",
    "\n",
    "#         result_data.tolist()\n",
    "\n",
    "\n",
    "#         data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/pkl/data.pkl.gz\",\"rb\"))\n",
    "#         print(\"data loaded!\")\n",
    "\n",
    "#         train_labels = data['train']['labels']\n",
    "#         train_sentences = data['train']['sentences']\n",
    "\n",
    "#         dev_labels = data['dev']['labels']\n",
    "#         dev_sentences = data['dev']['sentences']\n",
    "\n",
    "#         test_labels = data['test']['labels']\n",
    "#         test_sentences = data['test']['sentences']\n",
    "\n",
    "#         word_embeddings = data['wordEmbeddings']\n",
    "\n",
    "\n",
    "#         # :: Find the longest sentence in our dataset ::\n",
    "#         max_sentence_len = 0\n",
    "#         for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "#             max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "#         print(\"Longest sentence: %d\" % max_sentence_len)\n",
    "\n",
    "#         batch_size = 50\n",
    "#         result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "\n",
    "#         result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)\n",
    "\n",
    "#         senti = result_y_pred.ravel()\n",
    "#         print (senti)\n",
    "\n",
    "#         if(senti[0]>=0.65):\n",
    "#             speak(\"The User is positive\")\n",
    "#             print(\"The User is positive about: \", topicList)\n",
    "#         elif(senti[0]>=0.35 and senti[0]<0.65):\n",
    "#             speak(\"The user is Neutral\")\n",
    "#             print(\"The user is Neutral about: \", topicList)\n",
    "#         else:\n",
    "#             speak(\"The user is negative\")\n",
    "#             print(\"The user is negative about: \", topicList)\n",
    "        \n",
    "#     else:\n",
    "#         speak(\"Sorry I could not understand that!\")\n",
    "\n",
    "# #Loop should end here----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens in test_Text: 8.39%\n"
     ]
    }
   ],
   "source": [
    "#--------------------excluding the speech recognition and text to speech concept from the previous implementation:\n",
    "\n",
    "\n",
    "####################### Preprocessing for sentiment analysis starts here---------------------------------------    \n",
    "\n",
    "resultAudio = 'Hi I just wanted to say thanks for the last years supply. The website is super easy to use communication is clear and excellent. The % renewable energy is important to me too and you\\'ve implemented it very nicely. Far nicer than large suppliers I have used in the past. I\\'m only moving due to a significantly cheaper still % renewable electricity offer with no standing charges this appeals as the house stands empty for a large percentage of the time. If it doesn\\'t work out I\\'ll be back :) Thanks Rowan. On January at : So Energy wrote: Hello Rowan Your So Hedgehog tariff is ending soon Account Number: We\\'re getting in touch to let you know that your tariff is due to end on Mar . To make sure that you’re on our best value tariff you do need to take action. Time to renew your fixed rate contract Great Value Tariff Guaranteed We like to keep things nice and simple. We only have fixed rate tariff at any time so there’s no confusion as to which tariff to choose and we have a Low Price Commitment so you know you’ll always get a great value green energy deal. Equal or seasonally adjusted fixed payments We’re the only energy supplier that gives you the option to pay a lower fixed amount during summer months when your usage is typically less and a higher amount during the winter months when you typically use more. For more information on this take a look at our Season Payments Guide A few things that make us different... % renewable energy We are proud that % of the electricity we supply is from renewable sources and at no extra cost. We\\'ve partnered with generators from across Great Britain so we can provide our customers with green energy. Take a look at where your electricity comes from. Choose where your energy comes from We\\'re the only energy supplier that lets you decide where we source your renewable electricity from. Being one of our customers means you can vote your favourite renewable energy source and the results will decide our electricity fuel mix each year. Vote for your favourite energy source It\\'s SO quick and easy to renew You can renew your contract in clicks. If you would like to move over to our latest fixed tariff when your So Hedgehog tariff ends click on the button below to get your quote. The tariff we offer is subject to change and the tariff you can renew on today may change before your current contract ends. If you don’t do anything If you do not want to take out a new fixed term contract with us you will be moved onto our So Out of Contract tariff when your So Hedgehog tariff ends. As you\\'re within the last days of your contract if you choose to switch away from us you will not be charged any early exit fees. For more information To \"Know your rights\" as an energy consumer you can contact the Citizen\\'s Advice Consumer Service (CACS) on or by visiting their website www.citizensadvice.org.ukenergy . Thanks The So Energy Team Like us on Facebook Follow us on Twitter Get in touch: We\\'re open Monday to Friday am till pm Address: Power Road London W PY Copyright © So Energy Trading Ltd All rights reserved'\n",
    "\n",
    "#1. However when my contract finishes on the 2/07/2018 I am moving to a contract which is substantially more expensive than my current deal therefore I prefer to switch to rates that are lower\n",
    "\n",
    "testwords = resultAudio.split()\n",
    "\n",
    "\n",
    "def createtestMatrix(sentence, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "\n",
    "\n",
    "    testMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "\n",
    "    targetWordIdx = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        wordCount += 1\n",
    "\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = unknownIdx\n",
    "            unknownWordCount += 1\n",
    "        testMatrix.append(wordIdx)\n",
    "    print(\"Unknown tokens in test_Text: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "\n",
    "    return testMatrix\n",
    "\n",
    "\n",
    "word2Idx_path = open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/word2Idx.pickle\", \"rb\")\n",
    "word2Idx = pkl.load(word2Idx_path)\n",
    "\n",
    "# if len(testwords)!=0:\n",
    "#     finalTest_matrix = createtestMatrix(testwords, word2Idx)\n",
    "# else:\n",
    "#     continue\n",
    "\n",
    "finalTest_matrix = createtestMatrix(testwords, word2Idx)\n",
    "\n",
    "resultFilePath = 'C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/resultdata.pkl.gz'\n",
    "\n",
    "testarray = np.array(finalTest_matrix)\n",
    "testarray = testarray.reshape(1,len(finalTest_matrix))\n",
    "\n",
    "testarray.tolist()\n",
    "testarray.shape\n",
    "\n",
    "f = gzip.open(resultFilePath, 'wb')\n",
    "pkl.dump(testarray, f)\n",
    "f.close()\n",
    "\n",
    "##########################pre-processing for the sentiment analysis ends here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################Preprocessing for topic modelling starts here\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [resultAudio]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "    # stem tokens\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    # add tokens to list\n",
    "    texts.append(stopped_tokens)\n",
    "\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "topicresult = ldamodel.print_topics(num_topics=10, num_words=5)\n",
    "topicList = []\n",
    "\n",
    "for j in range(len(topicresult)):\n",
    "    topic = topicresult[j][1]\n",
    "# topic = topic.strip('\\\"')\n",
    "    x = topic.split(\"+\")\n",
    "    a = []\n",
    "    words=[]\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        b = x[i][6:].strip(' ')\n",
    "        b = b.strip('*')\n",
    "        b = b.strip('\"')\n",
    "        words.append(b)\n",
    "        pos = nltk.pos_tag(words)\n",
    "#         print(b)\n",
    "        if pos[0][1][0]=='N' or pos[0][1][0]=='V':\n",
    "            a.append(b)\n",
    "        words = []\n",
    "        pos=[]\n",
    "#topicList will have all the topics if the num_topics>1        \n",
    "topicList.append(a)\n",
    "\n",
    "str1 =  \" \".join(stri for stri in a)\n",
    "\n",
    "##########################preprocessing for topic modelling ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aravindhan.Poopathy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data loaded!\n",
      "data loaded!\n",
      "Longest sentence: 704\n",
      "[[0.31132185 0.61794794 0.04518349 0.02291064 0.00263613]]\n"
     ]
    }
   ],
   "source": [
    "############################Deeplearning model:\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "loadedModel = load_model(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/soemailModels/classMailModel1.h5\")\n",
    "\n",
    "result_data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "print(\"input data loaded!\")\n",
    "\n",
    "result_data.tolist()\n",
    "\n",
    "\n",
    "data = pkl.load(gzip.open(\"C:/Users/Aravindhan.Poopathy/OneDrive - So Energy/Arav/Sentence_classic_new-master/Sentence_classic_new-master/code/soeMailData/pkl/maildata.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")\n",
    "\n",
    "train_labels = data['train']['labels']\n",
    "train_sentences = data['train']['sentences']\n",
    "\n",
    "dev_labels = data['dev']['labels']\n",
    "dev_sentences = data['dev']['sentences']\n",
    "\n",
    "test_labels = data['test']['labels']\n",
    "test_sentences = data['test']['sentences']\n",
    "\n",
    "word_embeddings = data['wordEmbeddings']\n",
    "\n",
    "\n",
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "print(\"Longest sentence: %d\" % max_sentence_len)\n",
    "\n",
    "batch_size = 50\n",
    "result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "\n",
    "result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)\n",
    "\n",
    "# senti = result_y_pred.ravel()\n",
    "# print (senti)\n",
    "\n",
    "print(result_y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['energy', 'tariff', 're', 'contract']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y_pred = result_y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31132185459136963,\n",
       " 0.6179479360580444,\n",
       " 0.04518349468708038,\n",
       " 0.02291063778102398,\n",
       " 0.002636126009747386]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result_y_pred[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6179479360580444\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "index, value = max(enumerate(result), key=operator.itemgetter(1))\n",
    "print(index,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user Rating is Good!  [['energy', 'tariff', 're', 'contract']]\n"
     ]
    }
   ],
   "source": [
    "if(index==0):\n",
    "    print(\"The User Rating is Excellent! \", topicList)\n",
    "elif(index==1):\n",
    "    print(\"The user Rating is Good! \", topicList)\n",
    "elif(index==2):\n",
    "    print(\"The user Rating is Average! \", topicList)\n",
    "elif(index==3):\n",
    "    print(\"The user Rating is Bad! \", topicList)\n",
    "elif(index==4):\n",
    "    print(\"The user Rating is Terrible \", topicList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
